{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c35d59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import display, HTML\n",
    "from collections import defaultdict\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(\"__file__\"), '..')))\n",
    "from datasets.Waymo import WaymoDataset, waymo_collate_fn\n",
    "from model import OccupancyFlowNetwork\n",
    "from visualize import render_observed_scene_state, render_flow_at_spacetime, render_flow_field, render_occupancy_and_flow_unoccluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09d5231e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SCENES = 16#25\n",
    "MAX_SCENES_TO_RENDER = 1\n",
    "\n",
    "tfrecord_path = '../../data1/waymo_dataset/v1.1/waymo_open_dataset_motion_v_1_1_0/uncompressed/tf_example/validation'\n",
    "idx_path = '../../data1/waymo_dataset/v1.1/idx/validation'\n",
    "dataset = WaymoDataset(tfrecord_path, idx_path)\n",
    "dataloader = DataLoader(dataset, batch_size=NUM_SCENES, collate_fn=waymo_collate_fn)\n",
    "\n",
    "scenes = []\n",
    "#for _ in range(NUM_SCENES):\n",
    "scenes.append(next(iter(dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ec54fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for road_map, agent_trajectories, _, _, _, _, _, _ in scenes:\n",
    "    count += 1\n",
    "    if count > MAX_SCENES_TO_RENDER:\n",
    "        break\n",
    "    \n",
    "    render_observed_scene_state(road_map[0], agent_trajectories[0])#, f'examples/observed_scene_state/sample{count}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25582bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for road_map, _, _, flow_field_positions, flow_field_times, flow_field_velocities, _, _ in scenes:\n",
    "    count += 1\n",
    "    if count > MAX_SCENES_TO_RENDER:\n",
    "        break\n",
    "    \n",
    "    anim = render_flow_at_spacetime(road_map[0], flow_field_times[0], flow_field_positions[0], flow_field_velocities[0])#, f'examples/ground_truth/sample{count}.gif')\n",
    "    display(HTML(anim.to_jshtml()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f99e7084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "road map: torch.Size([16, 256, 256, 3])\n",
      "agent trajectories: torch.Size([16, 64, 11, 10])\n",
      "flow field agent ids: torch.Size([16, 69975, 1])\n",
      "flow field positions: torch.Size([16, 69975, 2])\n",
      "flow field times: torch.Size([16, 69975, 1])\n",
      "flow field velocities: torch.Size([16, 69975, 2])\n",
      "agent mask: torch.Size([16, 64])\n",
      "flow field mask: torch.Size([16, 69975])\n",
      "tensor(4.9543, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(4.9543, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "def end_point_error(target_flow, estimated_flow, mask=None):\n",
    "    l2_distance = torch.norm(estimated_flow - target_flow, p=2, dim=-1)\n",
    "    if mask is not None:\n",
    "        l2_distance = l2_distance * mask\n",
    "        sum_per_scene = l2_distance.sum(dim=-1)\n",
    "        count_per_scene = mask.sum(dim=-1)\n",
    "        scene_epe = sum_per_scene / count_per_scene\n",
    "        epe = scene_epe.mean()\n",
    "    else:\n",
    "        epe = l2_distance.mean()\n",
    "    return epe\n",
    "\n",
    "flow_field = OccupancyFlowNetwork(road_map_image_size=256, road_map_window_size=8, \n",
    "                                  trajectory_feature_dim=10, \n",
    "                                  embedding_dim=256, \n",
    "                                  flow_field_hidden_dim=256, flow_field_fourier_features=0).to(device)\n",
    "flow_field.eval()\n",
    "\n",
    "road_map, agent_trajectories, flow_field_agent_ids, flow_field_positions, flow_field_times, flow_field_velocities, agent_mask, flow_field_mask = scenes[0]\n",
    "road_map = road_map.to(device)\n",
    "agent_trajectories = agent_trajectories.to(device)\n",
    "flow_field_agent_ids = flow_field_agent_ids.to(device)\n",
    "flow_field_positions = flow_field_positions.to(device)\n",
    "flow_field_times = flow_field_times.to(device)\n",
    "flow_field_velocities = flow_field_velocities.to(device)\n",
    "agent_mask = agent_mask.to(device)\n",
    "flow_field_mask = flow_field_mask.to(device)\n",
    "\n",
    "print(f'road map: {road_map.shape}')\n",
    "print(f'agent trajectories: {agent_trajectories.shape}')\n",
    "print(f'flow field agent ids: {flow_field_agent_ids.shape}')\n",
    "print(f'flow field positions: {flow_field_positions.shape}')\n",
    "print(f'flow field times: {flow_field_times.shape}')\n",
    "print(f'flow field velocities: {flow_field_velocities.shape}')\n",
    "print(f'agent mask: {agent_mask.shape}')\n",
    "print(f'flow field mask: {flow_field_mask.shape}')\n",
    "\n",
    "result1 = 0\n",
    "count = 0\n",
    "for scene_idx in range(road_map.shape[0]):\n",
    "    scene_road_map = road_map[scene_idx].unsqueeze(0)\n",
    "    scene_agent_trajectories = agent_trajectories[scene_idx].unsqueeze(0)\n",
    "    scene_flow_field_agent_ids = flow_field_agent_ids[scene_idx].unsqueeze(0)\n",
    "    scene_flow_field_positions = flow_field_positions[scene_idx].unsqueeze(0)\n",
    "    scene_flow_field_times = flow_field_times[scene_idx].unsqueeze(0)\n",
    "    scene_flow_field_velocities = flow_field_velocities[scene_idx].unsqueeze(0)\n",
    "    scene_agent_mask = agent_mask[scene_idx].unsqueeze(0)\n",
    "    scene_flow_field_mask = flow_field_mask[scene_idx].unsqueeze(0)\n",
    "\n",
    "    #scene_context = torch.zeros(1, 256).to(device)\n",
    "    scene_estimated_flow, scene_context = flow_field(scene_flow_field_times, scene_flow_field_positions, scene_road_map, scene_agent_trajectories, scene_agent_mask)\n",
    "    #print(scene_context)\n",
    "    #scene_estimated_flow = flow_field.flow_field(scene_flow_field_times, scene_flow_field_positions, scene_context)\n",
    "    #scene_estimated_flow = scene_estimated_flow[scene_flow_field_mask]\n",
    "    #scene_flow_field_velocities = scene_flow_field_velocities[scene_flow_field_mask]\n",
    "    \n",
    "    # l2_dist = torch.norm(scene_estimated_flow - scene_flow_field_velocities, p=2, dim=-1)\n",
    "    # l2_dist = l2_dist * scene_flow_field_mask\n",
    "    # sum_per_scene = l2_dist.sum(dim=-1)\n",
    "    # count_per_scene = scene_flow_field_mask.sum(dim=-1)\n",
    "    # scene_epe = sum_per_scene / count_per_scene\n",
    "    # epe = scene_epe.mean()\n",
    "    epe = end_point_error(scene_flow_field_velocities, scene_estimated_flow, scene_flow_field_mask)\n",
    "    result1 += epe\n",
    "    count += 1\n",
    "    #result1 += torch.sum(scene_estimated_flow)\n",
    "result1 = result1 / count\n",
    "\n",
    "#scene_context = torch.zeros(16, 256).to(device)\n",
    "estimated_flow, scene_context = flow_field(flow_field_times, flow_field_positions, road_map, agent_trajectories, agent_mask)\n",
    "#estimated_flow = flow_field.flow_field(flow_field_times, flow_field_positions, scene_context)\n",
    "#estimated_flow = estimated_flow[flow_field_mask]\n",
    "#result2 = torch.sum(estimated_flow)\n",
    "\n",
    "#flow_field_velocities = flow_field_velocities[flow_field_mask]\n",
    "#l2_dist = torch.norm(estimated_flow - flow_field_velocities, p=2, dim=-1)\n",
    "#epe = l2_dist.mean()\n",
    "# l2_dist = torch.norm(estimated_flow - flow_field_velocities, p=2, dim=-1)\n",
    "# l2_dist = l2_dist * flow_field_mask\n",
    "# sum_per_scene = l2_dist.sum(dim=-1)\n",
    "# count_per_scene = flow_field_mask.sum(dim=-1)\n",
    "# scene_epe = sum_per_scene / count_per_scene\n",
    "# epe = scene_epe.mean()\n",
    "epe = end_point_error(estimated_flow, flow_field_velocities, flow_field_mask)\n",
    "result2 = epe\n",
    "\n",
    "#print('>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "#print(scene_context)\n",
    "\n",
    "print(result1)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1db3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def occupancy_alignment(flow_field, scene_context,\n",
    "                        agent_ids, positions, times):\n",
    "    occupancy_loss = 0\n",
    "    count = 0\n",
    "\n",
    "    agent_groups = defaultdict(list)\n",
    "    [agent_groups[round(val.item(), 1)].append(idx) for idx, val in enumerate(agent_ids)]\n",
    "\n",
    "    for id, agent_indices in agent_groups.items():#agent_groups.values():\n",
    "        agent_poistions = positions[agent_indices]\n",
    "        agent_times = times[agent_indices]\n",
    "        \n",
    "        time_groups = defaultdict(list)\n",
    "        [time_groups[round(val.item(), 1)].append(idx) for idx, val in enumerate(agent_times)]\n",
    "\n",
    "        occupancy = []\n",
    "        integration_times = []\n",
    "\n",
    "        for time, time_indices in time_groups.items():\n",
    "            integration_times.append(time)\n",
    "            occupancy.append(agent_poistions[time_indices])\n",
    "\n",
    "        initial_value = occupancy[0].unsqueeze(0) # TODO: unsqueeze is weird here we only do it because of ode expected shape...\n",
    "        integration_times = torch.FloatTensor(integration_times).to(times.device)\n",
    "        estimated_occupancy = flow_field.warp_occupancy(initial_value, integration_times, scene_context, use_custom=True)\n",
    "\n",
    "        for i in range(len(occupancy)):\n",
    "            occupancy_loss += torch.mean(torch.abs(estimated_occupancy[i].squeeze(0) - occupancy[i]))\n",
    "            count += 1\n",
    "            \n",
    "    return occupancy_loss / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fb227c",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_field = OccupancyFlowNetwork(road_map_image_size=256, road_map_window_size=8, \n",
    "                                  trajectory_feature_dim=10, \n",
    "                                  embedding_dim=256, \n",
    "                                  flow_field_hidden_dim=256, flow_field_fourier_features=0).to(device)\n",
    "flow_field.train()\n",
    "\n",
    "\n",
    "optim = torch.optim.Adam(flow_field.parameters(), lr=1e-4, weight_decay=0)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.999)\n",
    "\n",
    "EPOCHS = 1000\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    for road_map, agent_trajectories, flow_field_agent_ids, flow_field_positions, flow_field_times, flow_field_velocities, agent_mask, flow_field_mask in scenes:\n",
    "        road_map = road_map.to(device)\n",
    "        agent_trajectories = agent_trajectories.to(device)\n",
    "        p = flow_field_positions.to(device)\n",
    "        t = flow_field_times.to(device)\n",
    "        v = flow_field_velocities.to(device)\n",
    "        agent_mask = agent_mask.to(device)\n",
    "        flow_field_mask = flow_field_mask.to(device)\n",
    "    \n",
    "        print(agent_trajectories[agent_mask].shape)\n",
    "\n",
    "        #scene_context = flow_field.scene_encoder(road_map, agent_trajectories)\n",
    "        #flow = flow_field.flow_field(t, p, scene_context)\n",
    "        flow, _ = flow_field(t, p, road_map, agent_trajectories, agent_mask)\n",
    "        flow_field_mask = flow_field_mask.view(-1)\n",
    "        flow = flow.view(-1, 2)[flow_field_mask == 1]\n",
    "        v = v.view(-1, 2)[flow_field_mask == 1]\n",
    "\n",
    "        flow_loss = F.mse_loss(flow, v)\n",
    "        loss = flow_loss\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(flow_field.parameters(), max_norm=1.0)\n",
    "        optim.step()\n",
    "\n",
    "        epoch_loss += loss\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    epoch_loss /= NUM_SCENES\n",
    "    \n",
    "    if epoch == 0 or (epoch + 1) % 100 == 0:\n",
    "        print(f'epoch {epoch+1} loss: {epoch_loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5f2e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for road_map, agent_trajectories, _, flow_field_positions, flow_field_times, _, _, flow_field_mask in scenes:\n",
    "    count += 1\n",
    "    if count > MAX_SCENES_TO_RENDER:\n",
    "        break\n",
    "\n",
    "    road_map = road_map.to(device)\n",
    "    agent_trajectories = agent_trajectories.to(device)\n",
    "    p = flow_field_positions.to(device)\n",
    "    t = flow_field_times.to(device)\n",
    "    flow = flow_field(t, p, road_map, agent_trajectories)\n",
    "\n",
    "    anim = render_flow_at_spacetime(road_map[0].cpu(), flow_field_times[0].cpu(), flow_field_positions[0].cpu(), flow[0].detach().cpu())\n",
    "    display(HTML(anim.to_jshtml()))\n",
    "\n",
    "#count = 0\n",
    "#for road_map, agent_trajectories, _, positions, times, _, _, _ in scenes:\n",
    "#    count += 1\n",
    "#    if count > MAX_SCENES_TO_RENDER:\n",
    "#        break\n",
    "\n",
    "#    road_map = road_map.to(device)\n",
    "#    agent_trajectories = agent_trajectories.to(device)\n",
    "#    positions = positions.to(device)\n",
    "#    times = times.to(device)\n",
    "    \n",
    "#    scene_context = flow_field.scene_encoder(road_map, agent_trajectories)\n",
    "#    anim = render_occupancy_and_flow_unoccluded(flow_field, road_map, times, positions, 11, scene_context)#, f'examples/unaligned_occupancy_estimate/sample{count}.gif')\n",
    "#    display(HTML(anim.to_jshtml()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af75653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: I wonder if this should be part of the waymo collate function\n",
    "# def construct_agent_trajectories(agent_ids, positions, times):\n",
    "#     #print(agent_ids.shape)\n",
    "#     #print(positions.shape)\n",
    "#     #print(times.shape)\n",
    "#     time_groups = defaultdict(list)\n",
    "#     [time_groups[round(val.item(), 1)].append(idx) for idx, val in enumerate(times)]\n",
    "#     integration_times = torch.FloatTensor(sorted([t for t in time_groups.keys() if t <= 9.0])).to(times.device)\n",
    "\n",
    "#     #trajectories = [{} for _ in range(len(time_groups.keys()))]\n",
    "#     trajectories = [{} for _ in range(91)]\n",
    "#     #present = [defaultdict(lambda: False) for _ in range(len(time_groups.keys()))]\n",
    "#     present = [defaultdict(lambda: False) for _ in range(91)]\n",
    "#     #initial_values = [[] for _ in range(len(time_groups.keys()))]\n",
    "#     initial_values = [[] for _ in range(91)]\n",
    "#     agent_seen = set()\n",
    "#     agent_offsets = {}\n",
    "#     offset = 0\n",
    "\n",
    "#     #print(time_groups.keys())\n",
    "#     for time in sorted(time_groups.keys()):\n",
    "#         time_indicies = time_groups[time]\n",
    "#         agent_ids_at_time = agent_ids[time_indicies]\n",
    "#         time_index = int(time * 10)\n",
    "\n",
    "#         agent_groups = defaultdict(list)\n",
    "#         #[agent_groups[int(val.item())].append(idx) for idx, val in enumerate(agent_ids_at_time)]\n",
    "#         for local_idx, val in enumerate(agent_ids_at_time):\n",
    "#             global_idx = time_indicies[local_idx]\n",
    "#             agent_groups[int(val.item())].append(global_idx)\n",
    "\n",
    "#         for id, agent_indicies in agent_groups.items():\n",
    "#             agent_positions_at_time = positions[agent_indicies]\n",
    "\n",
    "#             #print(f'{time_index}-{len(trajectories)}')\n",
    "#             if time_index < len(trajectories): # only train up to the forecast horizon\n",
    "#                 trajectories[time_index][id] = agent_positions_at_time\n",
    "#                 present[time_index][id] = True\n",
    "            \n",
    "#                 if id not in agent_seen:\n",
    "#                     agent_seen.add(id)\n",
    "#                     initial_values[time_index].append(agent_positions_at_time)\n",
    "#                     num_agent_positions = agent_positions_at_time.shape[0]\n",
    "#                     start = offset\n",
    "#                     end = offset + num_agent_positions\n",
    "#                     agent_offsets[id] = (start, end)\n",
    "#                     offset = end\n",
    "\n",
    "#     return trajectories, present, initial_values, agent_offsets, integration_times, list(agent_seen)\n",
    "\n",
    "# def construct_agent_trajectories2(agent_ids, positions, times):\n",
    "#     rounded_times = torch.round(times * 10) / 10.0\n",
    "#     unique_times = torch.unique(rounded_times)\n",
    "#     integration_times = unique_times[unique_times <= 9.0]\n",
    "\n",
    "#     num_bins = 91\n",
    "#     trajectories = [{} for _ in range(num_bins)]\n",
    "#     present = [defaultdict(lambda: False) for _ in range(num_bins)]\n",
    "#     initial_values = [[] for _ in range(num_bins)]\n",
    "\n",
    "#     agent_seen = set()\n",
    "#     agent_offsets = {}\n",
    "#     offset = 0\n",
    "\n",
    "#     for time_val in integration_times:\n",
    "#         mask = rounded_times == time_val\n",
    "#         time_indices = torch.nonzero(mask.flatten(), as_tuple=False).squeeze(1)\n",
    "#         agent_ids_at_time = agent_ids[time_indices]\n",
    "#         time_index = int((time_val * 10).item())\n",
    "\n",
    "#         unique_ids = torch.unique(agent_ids_at_time)\n",
    "#         for agent_id in unique_ids.tolist():\n",
    "#             agent_mask = agent_ids_at_time == agent_id\n",
    "#             agent_indices = torch.nonzero(agent_mask.flatten(), as_tuple=False).squeeze(1)\n",
    "#             global_indices = time_indices[agent_indices]\n",
    "#             agent_positions = positions[global_indices]\n",
    "\n",
    "#             trajectories[time_index][agent_id] = agent_positions\n",
    "#             present[time_index][agent_id] = True\n",
    "\n",
    "#             if agent_id not in agent_seen:\n",
    "#                 agent_seen.add(agent_id)\n",
    "#                 initial_values[time_index].append(agent_positions)\n",
    "#                 num_agent_positions = agent_positions.shape[0]\n",
    "#                 start = offset\n",
    "#                 end = offset + num_agent_positions\n",
    "#                 agent_offsets[agent_id] = (start, end)\n",
    "#                 offset = end\n",
    "\n",
    "#     return trajectories, present, initial_values, agent_offsets, integration_times, list(agent_seen)\n",
    "\n",
    "# def reconstruct_trajectories(estimated_occupancy, present, agent_offsets, integration_times, agent_ids):\n",
    "#     #reconstructed_trajectories = [{} for _ in range(integration_times.shape[0])]\n",
    "#     reconstructed_trajectories = [{} for _ in range(91)]\n",
    "#     for time_index, _ in enumerate(integration_times):\n",
    "#         for id in agent_ids:\n",
    "#             if present[time_index][id]:\n",
    "#                 start, end = agent_offsets[id]\n",
    "#                 estimated_occupancy_at_time = estimated_occupancy[time_index][0]\n",
    "#                 reconstructed_trajectories[time_index][id] = estimated_occupancy_at_time[start:end]\n",
    "#     return reconstructed_trajectories\n",
    "\n",
    "# def occupancy_alignment2(flow_field, scene_context,\n",
    "#                          #agent_ids, positions, times):\n",
    "#                          trajectories, present, initial_values, agent_offsets, integration_times, ids):\n",
    "#     #trajectories, present, initial_values, agent_offsets, integration_times, ids = construct_agent_trajectories(agent_ids, positions, times)\n",
    "#     estimated_occupancy = flow_field.warp_occupancy(initial_values, integration_times, scene_context)\n",
    "#     estimated_trajectories = reconstruct_trajectories(estimated_occupancy, present, agent_offsets, integration_times, ids)\n",
    "    \n",
    "#     occupancy_loss = 0\n",
    "#     count = 0\n",
    "\n",
    "#     for time_index, _ in enumerate(integration_times):\n",
    "#         for id in ids:\n",
    "#             if present[time_index][id]:\n",
    "#                 ground_truth_positions = trajectories[time_index][id]\n",
    "#                 estimated_positions = estimated_trajectories[time_index][id]\n",
    "#                 occupancy_loss += torch.mean(torch.abs(ground_truth_positions - estimated_positions))\n",
    "#                 count += 1\n",
    "                \n",
    "#     return occupancy_loss / count\n",
    "\n",
    "# def occupancy_alignment3(flow_field, scene_context,\n",
    "#                          agent_ids, positions, times):\n",
    "#     trajectories, present, initial_values, agent_offsets, integration_times, ids = construct_agent_trajectories2(agent_ids, positions, times)\n",
    "#     estimated_occupancy = flow_field.flow_field.solve_ivp(initial_values, integration_times, scene_context)\n",
    "#     estimated_trajectories = reconstruct_trajectories(estimated_occupancy, present, agent_offsets, integration_times, ids)\n",
    "    \n",
    "#     occupancy_loss = 0\n",
    "#     count = 0\n",
    "\n",
    "#     for time_index, _ in enumerate(integration_times):\n",
    "#         for id in ids:\n",
    "#             if present[time_index][id]:\n",
    "#                 ground_truth_positions = trajectories[time_index][id]\n",
    "#                 estimated_positions = estimated_trajectories[time_index][id]\n",
    "#                 occupancy_loss += torch.mean(torch.abs(ground_truth_positions - estimated_positions))\n",
    "#                 count += 1\n",
    "                \n",
    "#     return occupancy_loss / count\n",
    "\n",
    "def construct_agent_trajectories(agent_ids, positions, times, forecast_horizon):\n",
    "    rounded_times = torch.round(times * 10) / 10.0\n",
    "    unique_times = torch.unique(rounded_times)\n",
    "    integration_times = unique_times[unique_times <= 9.0]\n",
    "\n",
    "    trajectories = [{} for _ in range(forecast_horizon)]\n",
    "    present = [defaultdict(lambda: False) for _ in range(forecast_horizon)]\n",
    "    initial_values = [[] for _ in range(forecast_horizon)]\n",
    "\n",
    "    agent_seen = set()\n",
    "    agent_offsets = {}\n",
    "    offset = 0\n",
    "\n",
    "    for time_val in integration_times:\n",
    "        mask = rounded_times == time_val\n",
    "        time_indices = torch.nonzero(mask.flatten(), as_tuple=False).squeeze(1)\n",
    "        agent_ids_at_time = agent_ids[time_indices]\n",
    "        time_index = int((time_val * 10).item())\n",
    "\n",
    "        unique_ids = torch.unique(agent_ids_at_time)\n",
    "        for agent_id in unique_ids.tolist():\n",
    "            agent_mask = agent_ids_at_time == agent_id\n",
    "            agent_indices = torch.nonzero(agent_mask.flatten(), as_tuple=False).squeeze(1)\n",
    "            global_indices = time_indices[agent_indices]\n",
    "            agent_positions = positions[global_indices]\n",
    "\n",
    "            trajectories[time_index][agent_id] = agent_positions\n",
    "            present[time_index][agent_id] = True\n",
    "\n",
    "            if agent_id not in agent_seen:\n",
    "                agent_seen.add(agent_id)\n",
    "                initial_values[time_index].append(agent_positions)\n",
    "                num_agent_positions = agent_positions.shape[0]\n",
    "                start = offset\n",
    "                end = offset + num_agent_positions\n",
    "                agent_offsets[agent_id] = (start, end)\n",
    "                offset = end\n",
    "\n",
    "    return trajectories, present, initial_values, agent_offsets, integration_times, list(agent_seen)\n",
    "\n",
    "def reconstruct_trajectories(estimated_occupancy, present, agent_offsets, integration_times, agent_ids, forecast_horizon):\n",
    "    reconstructed_trajectories = [{} for _ in range(forecast_horizon)]\n",
    "    for time_index, _ in enumerate(integration_times):\n",
    "        for id in agent_ids:\n",
    "            if present[time_index][id]:\n",
    "                start, end = agent_offsets[id]\n",
    "                estimated_occupancy_at_time = estimated_occupancy[time_index][0]\n",
    "                reconstructed_trajectories[time_index][id] = estimated_occupancy_at_time[start:end]\n",
    "    return reconstructed_trajectories\n",
    "\n",
    "def occupancy_alignment(model, agent_ids, positions, times, flow_field_mask, scene_context, forecast_horizon=91):\n",
    "    loss = 0\n",
    "    count = 0\n",
    "\n",
    "    num_scenes = agent_ids.shape[0]\n",
    "    for scene_index in range(num_scenes):\n",
    "        scene_mask = flow_field_mask[scene_index]\n",
    "        ids = agent_ids[scene_index][scene_mask]\n",
    "        p = positions[scene_index][scene_mask]\n",
    "        t = times[scene_index][scene_mask]\n",
    "        context = scene_context[scene_index].unsqueeze(0)\n",
    "\n",
    "        loss += scene_occupancy_alignment(model, ids, p, t, context, forecast_horizon)\n",
    "        count += 1\n",
    "    \n",
    "    return loss / count\n",
    "\n",
    "def scene_occupancy_alignment(model, agent_ids, positions, times, scene_context, forecast_horizon=91):\n",
    "    trajectories, present, initial_values, agent_offsets, integration_times, ids = construct_agent_trajectories(agent_ids, positions, times, forecast_horizon)\n",
    "    estimated_occupancy = model.flow_field.solve_ivp(initial_values, integration_times, scene_context)\n",
    "    estimated_trajectories = reconstruct_trajectories(estimated_occupancy, present, agent_offsets, integration_times, ids, forecast_horizon)\n",
    "    \n",
    "    loss = 0\n",
    "    count = 0\n",
    "\n",
    "    for time_index, _ in enumerate(integration_times):\n",
    "        for id in ids:\n",
    "            if present[time_index][id]:\n",
    "                ground_truth_positions = trajectories[time_index][id]\n",
    "                estimated_positions = estimated_trajectories[time_index][id]\n",
    "                loss += torch.mean(torch.abs(ground_truth_positions - estimated_positions))\n",
    "                count += 1\n",
    "                \n",
    "    return loss / count\n",
    "\n",
    "for param in flow_field.scene_encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "optim = torch.optim.Adam(flow_field.flow_field.parameters(), lr=1e-5, weight_decay=0)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.999)\n",
    "\n",
    "# occupancy_alignment_tensors = []\n",
    "# with torch.no_grad():\n",
    "#     for road_map, agent_trajectories, flow_field_agent_ids, flow_field_positions, flow_field_times, flow_field_velocities, _, _ in scenes:\n",
    "#         road_map = road_map.to(device)\n",
    "#         agent_trajectories = agent_trajectories.to(device)\n",
    "#         p = flow_field_positions.to(device)\n",
    "#         t = flow_field_times.to(device)\n",
    "#         v = flow_field_velocities.to(device)\n",
    "    \n",
    "#         scene_context = flow_field.scene_encoder(road_map, agent_trajectories)\n",
    "#         trajectories, present, initial_values, agent_offsets, integration_times, ids = construct_agent_trajectories(flow_field_agent_ids[0], p[0], t[0])\n",
    "#         occupancy_alignment_tensors.append((trajectories, present, initial_values, agent_offsets, integration_times, ids, scene_context))\n",
    "\n",
    "EPOCHS = 1000\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_flow_loss = 0\n",
    "    epoch_occupancy_loss = 0\n",
    "    epoch_loss = 0\n",
    "    #for road_map, agent_trajectories, flow_field_agent_ids, flow_field_positions, flow_field_times, flow_field_velocities, _, _ in scenes:\n",
    "    for i in range(len(scenes)):\n",
    "        road_map, agent_trajectories, flow_field_agent_ids, flow_field_positions, flow_field_times, flow_field_velocities, _, flow_field_mask = scenes[i]\n",
    "        #trajectories, present, initial_values, agent_offsets, integration_times, ids, scene_context = occupancy_alignment_tensors[i]\n",
    "\n",
    "        road_map = road_map.to(device)\n",
    "        agent_trajectories = agent_trajectories.to(device)\n",
    "        flow_field_agent_ids = flow_field_agent_ids.to(device)\n",
    "        p = flow_field_positions.to(device)\n",
    "        t = flow_field_times.to(device)\n",
    "        v = flow_field_velocities.to(device)\n",
    "        flow_field_mask = flow_field_mask.to(device)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            scene_context = flow_field.scene_encoder(road_map, agent_trajectories)\n",
    "\n",
    "        flow = flow_field.flow_field(t, p, scene_context)\n",
    "\n",
    "        flow_loss = F.mse_loss(flow, v)\n",
    "\n",
    "        #occupancy_loss = occupancy_alignment2(flow_field, scene_context,\n",
    "        #                                      trajectories, present, initial_values, agent_offsets, integration_times, ids)\n",
    "        #occupancy_loss = occupancy_alignment3(flow_field, scene_context, flow_field_agent_ids[0], p[0], t[0])\n",
    "        occupancy_loss = occupancy_alignment(flow_field, flow_field_agent_ids, p, t, flow_field_mask, scene_context, forecast_horizon=91)\n",
    "\n",
    "        #print(f'ol2: {occupancy_loss}')\n",
    "        #print(f'ol3: {ol3}')\n",
    "\n",
    "        loss = flow_loss + occupancy_loss\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(flow_field.parameters(), max_norm=1.0)\n",
    "        optim.step()\n",
    "\n",
    "        epoch_flow_loss += flow_loss\n",
    "        epoch_occupancy_loss += occupancy_loss\n",
    "        epoch_loss += loss\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    epoch_flow_loss /= NUM_SCENES\n",
    "    epoch_occupancy_loss /= NUM_SCENES\n",
    "    epoch_loss /= NUM_SCENES\n",
    "    \n",
    "    #if epoch == 0 or (epoch + 1) % 100 == 0:\n",
    "    print(f'epoch {epoch+1} flow_loss: {epoch_flow_loss.item()}')\n",
    "    print(f'epoch {epoch+1} occupancy_loss: {epoch_occupancy_loss.item()}')\n",
    "    print(f'epoch {epoch+1} loss: {epoch_loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbeecf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for road_map, agent_trajectories, _, flow_field_positions, flow_field_times, _, _, _ in scenes:\n",
    "    count += 1\n",
    "    if count > MAX_SCENES_TO_RENDER:\n",
    "        break\n",
    "\n",
    "    road_map = road_map.to(device)\n",
    "    agent_trajectories = agent_trajectories.to(device)\n",
    "    p = flow_field_positions.to(device)\n",
    "    t = flow_field_times.to(device)\n",
    "    flow = flow_field(t, p, road_map, agent_trajectories)\n",
    "\n",
    "    anim = render_flow_at_spacetime(road_map[0].cpu(), flow_field_times[0].cpu(), flow_field_positions[0].cpu(), flow[0].detach().cpu())\n",
    "    display(HTML(anim.to_jshtml()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff79e7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for road_map, agent_trajectories, _, _, _, _, _, _ in scenes:\n",
    "    count += 1\n",
    "    if count > MAX_SCENES_TO_RENDER:\n",
    "        break\n",
    "\n",
    "    road_map = road_map.to(device)\n",
    "    agent_trajectories = agent_trajectories.to(device)\n",
    "    \n",
    "    scene_context = flow_field.scene_encoder(road_map, agent_trajectories)\n",
    "    anim = render_flow_field(flow_field, road_map, road_map[0].shape[0], 10, 91, 10, scene_context)\n",
    "    display(HTML(anim.to_jshtml()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76b6558",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for road_map, agent_trajectories, _, positions, times, _, _, _ in scenes:\n",
    "    count += 1\n",
    "    if count > MAX_SCENES_TO_RENDER:\n",
    "        break\n",
    "\n",
    "    road_map = road_map.to(device)\n",
    "    agent_trajectories = agent_trajectories.to(device)\n",
    "    positions = positions.to(device)\n",
    "    times = times.to(device)\n",
    "    \n",
    "    scene_context = flow_field.scene_encoder(road_map, agent_trajectories)\n",
    "    anim = render_occupancy_and_flow_unoccluded(flow_field, road_map, times, positions, 11, scene_context, f'examples/aligned_occupancy_estimate/sample{count}.gif')\n",
    "    #display(HTML(anim.to_jshtml()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ofenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
