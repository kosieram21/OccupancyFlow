{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c35d59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import display, HTML\n",
    "from collections import defaultdict\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(\"__file__\"), '..')))\n",
    "from datasets.Waymo import WaymoDataset, waymo_collate_fn, get_world_coordinates\n",
    "from model import OccupancyFlowNetwork\n",
    "from visualize import render_observed_scene_state, render_flow_at_spacetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d5231e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SCENES = 1\n",
    "MAX_SCENES_TO_RENDER = 1\n",
    "\n",
    "tfrecord_path = '../../data1/waymo_dataset/v1.1/waymo_open_dataset_motion_v_1_1_0/uncompressed/tf_example/validation'\n",
    "idx_path = '../../data1/waymo_dataset/v1.1/idx/validation'\n",
    "dataset = WaymoDataset(tfrecord_path, idx_path)\n",
    "dataloader = DataLoader(dataset, batch_size=1, collate_fn=waymo_collate_fn)\n",
    "\n",
    "scenes = []\n",
    "for _ in range(NUM_SCENES):\n",
    "    scenes.append(next(iter(dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ec54fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for road_map, agent_trajectories, _, _, _, _, _, _ in scenes:\n",
    "    count += 1\n",
    "    if count > MAX_SCENES_TO_RENDER:\n",
    "        break\n",
    "    \n",
    "    render_observed_scene_state(road_map[0], agent_trajectories[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25582bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for road_map, _, _, flow_field_positions, flow_field_times, flow_field_velocities, _, _ in scenes:\n",
    "    count += 1\n",
    "    if count > MAX_SCENES_TO_RENDER:\n",
    "        break\n",
    "    \n",
    "    anim = render_flow_at_spacetime(road_map[0], flow_field_times[0], flow_field_positions[0], flow_field_velocities[0])\n",
    "    display(HTML(anim.to_jshtml()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99e7084",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1db3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def occupancy_alignment(flow_field, scene_context,\n",
    "                        agent_ids, positions, times):\n",
    "    occupancy_loss = 0\n",
    "    count = 0\n",
    "\n",
    "    agent_groups = defaultdict(list)\n",
    "    [agent_groups[round(val.item(), 1)].append(idx) for idx, val in enumerate(agent_ids)]\n",
    "\n",
    "    for indices in agent_groups.values():\n",
    "        agent_poistions = positions[indices]\n",
    "        agent_times = times[indices]\n",
    "        \n",
    "        time_groups = defaultdict(list)\n",
    "        [time_groups[round(val.item(), 1)].append(idx) for idx, val in enumerate(agent_times)]\n",
    "\n",
    "        occupancy = []\n",
    "        integration_times = []\n",
    "\n",
    "        for time, indices in time_groups.items():\n",
    "            integration_times.append(time)\n",
    "            occupancy.append(agent_poistions[indices])\n",
    "\n",
    "        initial_value = occupancy[0].unsqueeze(0)\n",
    "        integration_times = torch.FloatTensor(integration_times).to(times.device)\n",
    "        estimated_occupancy = flow_field.warp_occupancy(initial_value, integration_times, scene_context)\n",
    "\n",
    "        #print('--------------------')\n",
    "        #for i in range(len(integration_times)):\n",
    "        #    print(f't: {integration_times[i]}')\n",
    "        #    print(f'ground truth occupancy: {occupancy[i][5]}')\n",
    "        #    print(f'estimated occupancy: {estimated_occupancy[i].squeeze(0)[5]}')\n",
    "        #print('--------------------')\n",
    "\n",
    "        for i in range(len(occupancy)):\n",
    "            occupancy_loss += F.mse_loss(estimated_occupancy[i].squeeze(0), occupancy[i])\n",
    "            count += 1\n",
    "\n",
    "    print(f'occupancy_loss: {occupancy_loss / count}')\n",
    "    return torch.clamp(occupancy_loss / count, max=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fb227c",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_field = OccupancyFlowNetwork(road_map_image_size=256, road_map_window_size=8, \n",
    "                                  trajectory_feature_dim=10, \n",
    "                                  embedding_dim=256, \n",
    "                                  flow_field_hidden_dim=256, flow_field_fourier_features=128).to(device)\n",
    "flow_field.train()\n",
    "\n",
    "\n",
    "optim = torch.optim.Adam(flow_field.parameters(), lr=1e-4, weight_decay=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.999)\n",
    "\n",
    "EPOCHS = 1000\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    for road_map, agent_trajectories, flow_field_agent_ids, flow_field_positions, flow_field_times, flow_field_velocities, _, _ in scenes:\n",
    "        road_map = road_map.to(device)\n",
    "        agent_trajectories = agent_trajectories.to(device)\n",
    "        p = flow_field_positions.to(device)\n",
    "        t = flow_field_times.to(device)\n",
    "        v = flow_field_velocities.to(device)\n",
    "    \n",
    "        scene_context = flow_field.scene_encoder(road_map, agent_trajectories)\n",
    "        flow = flow_field.flow_field(t, p, scene_context)\n",
    "        #print(\"Flow norm:\", flow.norm(dim=-1).mean().item())\n",
    "\n",
    "        flow_loss = F.mse_loss(flow, v)\n",
    "        \n",
    "        if epoch > 500:\n",
    "            occupancy_loss = occupancy_alignment(flow_field, scene_context,\n",
    "                                                 flow_field_agent_ids[0], p[0], t[0])\n",
    "            loss = flow_loss + occupancy_loss\n",
    "        else:\n",
    "            loss = flow_loss\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(flow_field.parameters(), max_norm=1.0)\n",
    "        optim.step()\n",
    "\n",
    "        epoch_loss += loss\n",
    "\n",
    "        #print(f'batch loss: {loss}')\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    epoch_loss /= (NUM_SCENES)\n",
    "    #if epoch == 0 or (epoch + 1) % 100 == 0:\n",
    "    print(f'epoch {epoch+1} loss: {epoch_loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbeecf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for road_map, agent_trajectories, _, flow_field_positions, flow_field_times, _, _, _ in scenes:\n",
    "    count += 1\n",
    "    if count > MAX_SCENES_TO_RENDER:\n",
    "        break\n",
    "\n",
    "    road_map = road_map.to(device)\n",
    "    agent_trajectories = agent_trajectories.to(device)\n",
    "    p = flow_field_positions.to(device)\n",
    "    t = flow_field_times.to(device)\n",
    "    flow = flow_field(t, p, road_map, agent_trajectories)\n",
    "\n",
    "    anim = render_flow_at_spacetime(road_map[0].cpu(), flow_field_times[0].cpu(), flow_field_positions[0].cpu(), flow[0].detach().cpu())\n",
    "    display(HTML(anim.to_jshtml()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c73ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRID_SIZE = road_map[0].shape[0]\n",
    "STRIDE = 10\n",
    "TIMESTEPS = 80\n",
    "FREQ = 10\n",
    "\n",
    "y_coords = np.arange(0, GRID_SIZE, STRIDE)\n",
    "x_coords = np.arange(0, GRID_SIZE, STRIDE)\n",
    "grid_x, grid_y = np.meshgrid(x_coords, y_coords)\n",
    "grid_points = np.column_stack((grid_x.flatten(), grid_y.flatten()))\n",
    "grid_points = get_world_coordinates(grid_points)\n",
    "grid_points = torch.FloatTensor(grid_points)\n",
    "\n",
    "num_cells = grid_points.shape[0]\n",
    "\n",
    "grid_points = grid_points.repeat(TIMESTEPS, 1)\n",
    "grid_points = grid_points.reshape(-1, 2).unsqueeze(0)\n",
    "\n",
    "grid_times = [1.1 + t / FREQ for t in range(TIMESTEPS)]\n",
    "grid_times = torch.FloatTensor(grid_times)\n",
    "grid_times = grid_times.repeat_interleave(num_cells).unsqueeze(0).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff79e7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for road_map, agent_trajectories, _, _, _, _, _, _ in scenes:\n",
    "    count += 1\n",
    "    if count > MAX_SCENES_TO_RENDER:\n",
    "        break\n",
    "\n",
    "    road_map = road_map.to(device)\n",
    "    agent_trajectories = agent_trajectories.to(device)\n",
    "    grid_times = grid_times.to(device)\n",
    "    grid_points = grid_points.to(device)\n",
    "    flow = flow_field(grid_times, grid_points, road_map, agent_trajectories)\n",
    "    \n",
    "    anim = render_flow_at_spacetime(road_map[0].cpu(), grid_times[0].cpu(), grid_points[0].cpu(), flow[0].detach().cpu())\n",
    "    display(HTML(anim.to_jshtml()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ofenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
