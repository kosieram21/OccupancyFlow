{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c35d59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import display, HTML\n",
    "from collections import defaultdict\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(\"__file__\"), '..')))\n",
    "from datasets.Waymo import WaymoDataset, waymo_collate_fn\n",
    "from model import OccupancyFlowNetwork\n",
    "from visualize import render_observed_scene_state, render_flow_at_spacetime, render_flow_field, render_occupancy_and_flow_unoccluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09d5231e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SCENES = 16#25\n",
    "MAX_SCENES_TO_RENDER = 0\n",
    "\n",
    "tfrecord_path = '../../data1/waymo_dataset/v1.1/waymo_open_dataset_motion_v_1_1_0/uncompressed/tf_example/validation'\n",
    "idx_path = '../../data1/waymo_dataset/v1.1/idx/validation'\n",
    "dataset = WaymoDataset(tfrecord_path, idx_path)\n",
    "dataloader = DataLoader(dataset, batch_size=1, collate_fn=waymo_collate_fn)\n",
    "\n",
    "scenes = []\n",
    "for _ in range(NUM_SCENES):\n",
    "    scenes.append(next(iter(dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0ec54fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for road_map, agent_trajectories, _, _, _, _, _, _ in scenes:\n",
    "    count += 1\n",
    "    if count > MAX_SCENES_TO_RENDER:\n",
    "        break\n",
    "    \n",
    "    render_observed_scene_state(road_map[0], agent_trajectories[0], f'examples/observed_scene_state/sample{count}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25582bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for road_map, _, _, flow_field_positions, flow_field_times, flow_field_velocities, _, _ in scenes:\n",
    "    count += 1\n",
    "    if count > MAX_SCENES_TO_RENDER:\n",
    "        break\n",
    "    \n",
    "    anim = render_flow_at_spacetime(road_map[0], flow_field_times[0], flow_field_positions[0], flow_field_velocities[0], f'examples/ground_truth/sample{count}.gif')\n",
    "    #display(HTML(anim.to_jshtml()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f99e7084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed1db3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def occupancy_alignment(flow_field, scene_context,\n",
    "                        agent_ids, positions, times):\n",
    "    occupancy_loss = 0\n",
    "    count = 0\n",
    "\n",
    "    agent_groups = defaultdict(list)\n",
    "    [agent_groups[round(val.item(), 1)].append(idx) for idx, val in enumerate(agent_ids)]\n",
    "\n",
    "    for id, agent_indices in agent_groups.items():#agent_groups.values():\n",
    "        agent_poistions = positions[agent_indices]\n",
    "        agent_times = times[agent_indices]\n",
    "        \n",
    "        time_groups = defaultdict(list)\n",
    "        [time_groups[round(val.item(), 1)].append(idx) for idx, val in enumerate(agent_times)]\n",
    "\n",
    "        occupancy = []\n",
    "        integration_times = []\n",
    "\n",
    "        for time, time_indices in time_groups.items():\n",
    "            integration_times.append(time)\n",
    "            occupancy.append(agent_poistions[time_indices])\n",
    "\n",
    "        initial_value = occupancy[0].unsqueeze(0) # TODO: unsqueeze is weird here we only do it because of ode expected shape...\n",
    "        integration_times = torch.FloatTensor(integration_times).to(times.device)\n",
    "        estimated_occupancy = flow_field.warp_occupancy(initial_value, integration_times, scene_context, use_custom=True)\n",
    "\n",
    "        for i in range(len(occupancy)):\n",
    "            occupancy_loss += torch.mean(torch.abs(estimated_occupancy[i].squeeze(0) - occupancy[i]))\n",
    "            count += 1\n",
    "            \n",
    "    return occupancy_loss / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05fb227c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 loss: 20.194156646728516\n",
      "epoch 100 loss: 3.9901602268218994\n",
      "epoch 200 loss: 2.582787036895752\n",
      "epoch 300 loss: 1.8644722700119019\n",
      "epoch 400 loss: 1.4894930124282837\n",
      "epoch 500 loss: 1.2943143844604492\n",
      "epoch 600 loss: 1.1046433448791504\n",
      "epoch 700 loss: 1.0349476337432861\n",
      "epoch 800 loss: 0.9298205971717834\n",
      "epoch 900 loss: 0.8698266744613647\n",
      "epoch 1000 loss: 0.8209153413772583\n"
     ]
    }
   ],
   "source": [
    "flow_field = OccupancyFlowNetwork(road_map_image_size=256, road_map_window_size=8, \n",
    "                                  trajectory_feature_dim=10, \n",
    "                                  embedding_dim=256, \n",
    "                                  flow_field_hidden_dim=256, flow_field_fourier_features=0).to(device)\n",
    "flow_field.train()\n",
    "\n",
    "\n",
    "optim = torch.optim.Adam(flow_field.parameters(), lr=1e-4, weight_decay=0)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.999)\n",
    "\n",
    "EPOCHS = 1000\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    for road_map, agent_trajectories, flow_field_agent_ids, flow_field_positions, flow_field_times, flow_field_velocities, _, _ in scenes:\n",
    "        road_map = road_map.to(device)\n",
    "        agent_trajectories = agent_trajectories.to(device)\n",
    "        p = flow_field_positions.to(device)\n",
    "        t = flow_field_times.to(device)\n",
    "        v = flow_field_velocities.to(device)\n",
    "    \n",
    "        scene_context = flow_field.scene_encoder(road_map, agent_trajectories)\n",
    "        flow = flow_field.flow_field(t, p, scene_context)\n",
    "\n",
    "        flow_loss = F.mse_loss(flow, v)\n",
    "        loss = flow_loss\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(flow_field.parameters(), max_norm=1.0)\n",
    "        optim.step()\n",
    "\n",
    "        epoch_loss += loss\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    epoch_loss /= NUM_SCENES\n",
    "    \n",
    "    if epoch == 0 or (epoch + 1) % 100 == 0:\n",
    "        print(f'epoch {epoch+1} loss: {epoch_loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af5f2e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for road_map, agent_trajectories, _, positions, times, _, _, _ in scenes:\n",
    "    count += 1\n",
    "    if count > MAX_SCENES_TO_RENDER:\n",
    "        break\n",
    "\n",
    "    road_map = road_map.to(device)\n",
    "    agent_trajectories = agent_trajectories.to(device)\n",
    "    positions = positions.to(device)\n",
    "    times = times.to(device)\n",
    "    \n",
    "    scene_context = flow_field.scene_encoder(road_map, agent_trajectories)\n",
    "    anim = render_occupancy_and_flow_unoccluded(flow_field, road_map, times, positions, 11, scene_context, f'examples/unaligned_occupancy_estimate/sample{count}.gif')\n",
    "    #display(HTML(anim.to_jshtml()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1af75653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 flow_loss: 0.7967304587364197\n",
      "epoch 1 occupancy_loss: 0.6630827188491821\n",
      "epoch 1 loss: 1.4598134756088257\n",
      "epoch 2 flow_loss: 0.7976434230804443\n",
      "epoch 2 occupancy_loss: 0.6481175422668457\n",
      "epoch 2 loss: 1.44576096534729\n",
      "epoch 3 flow_loss: 0.7908350825309753\n",
      "epoch 3 occupancy_loss: 0.6252407431602478\n",
      "epoch 3 loss: 1.4160757064819336\n",
      "epoch 4 flow_loss: 0.7932879328727722\n",
      "epoch 4 occupancy_loss: 0.6348521113395691\n",
      "epoch 4 loss: 1.4281400442123413\n",
      "epoch 5 flow_loss: 0.7863776087760925\n",
      "epoch 5 occupancy_loss: 0.6287205815315247\n",
      "epoch 5 loss: 1.4150981903076172\n",
      "epoch 6 flow_loss: 0.7903540730476379\n",
      "epoch 6 occupancy_loss: 0.6081734299659729\n",
      "epoch 6 loss: 1.3985272645950317\n",
      "epoch 7 flow_loss: 0.7883592247962952\n",
      "epoch 7 occupancy_loss: 0.6211004853248596\n",
      "epoch 7 loss: 1.4094595909118652\n",
      "epoch 8 flow_loss: 0.7837441563606262\n",
      "epoch 8 occupancy_loss: 0.6151504516601562\n",
      "epoch 8 loss: 1.3988945484161377\n",
      "epoch 9 flow_loss: 0.7894085645675659\n",
      "epoch 9 occupancy_loss: 0.5943561792373657\n",
      "epoch 9 loss: 1.3837647438049316\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 186\u001b[39m\n\u001b[32m    182\u001b[39m flow_loss = F.mse_loss(flow, v)\n\u001b[32m    184\u001b[39m \u001b[38;5;66;03m#occupancy_loss = occupancy_alignment2(flow_field, scene_context,\u001b[39;00m\n\u001b[32m    185\u001b[39m \u001b[38;5;66;03m#                                      trajectories, present, initial_values, agent_offsets, integration_times, ids)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m occupancy_loss = \u001b[43moccupancy_alignment3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflow_field\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscene_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflow_field_agent_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[38;5;66;03m#print(f'ol2: {occupancy_loss}')\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[38;5;66;03m#print(f'ol3: {ol3}')\u001b[39;00m\n\u001b[32m    191\u001b[39m loss = flow_loss + occupancy_loss\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 125\u001b[39m, in \u001b[36moccupancy_alignment3\u001b[39m\u001b[34m(flow_field, scene_context, agent_ids, positions, times)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moccupancy_alignment3\u001b[39m(flow_field, scene_context,\n\u001b[32m    124\u001b[39m                          agent_ids, positions, times):\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     trajectories, present, initial_values, agent_offsets, integration_times, ids = \u001b[43mconstruct_agent_trajectories2\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m     estimated_occupancy = flow_field.warp_occupancy2(initial_values, integration_times, scene_context)\n\u001b[32m    127\u001b[39m     estimated_trajectories = reconstruct_trajectories(estimated_occupancy, present, agent_offsets, integration_times, ids)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 74\u001b[39m, in \u001b[36mconstruct_agent_trajectories2\u001b[39m\u001b[34m(agent_ids, positions, times)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m agent_id \u001b[38;5;129;01min\u001b[39;00m unique_ids.tolist():\n\u001b[32m     73\u001b[39m     agent_mask = agent_ids_at_time == agent_id\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     agent_indices = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnonzero\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent_mask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_tuple\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m.squeeze(\u001b[32m1\u001b[39m)\n\u001b[32m     75\u001b[39m     global_indices = time_indices[agent_indices]\n\u001b[32m     76\u001b[39m     agent_positions = positions[global_indices]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# TODO: I wonder if this should be part of the waymo collate function\n",
    "def construct_agent_trajectories(agent_ids, positions, times):\n",
    "    #print(agent_ids.shape)\n",
    "    #print(positions.shape)\n",
    "    #print(times.shape)\n",
    "    time_groups = defaultdict(list)\n",
    "    [time_groups[round(val.item(), 1)].append(idx) for idx, val in enumerate(times)]\n",
    "    integration_times = torch.FloatTensor(sorted([t for t in time_groups.keys() if t <= 9.0])).to(times.device)\n",
    "\n",
    "    #trajectories = [{} for _ in range(len(time_groups.keys()))]\n",
    "    trajectories = [{} for _ in range(91)]\n",
    "    #present = [defaultdict(lambda: False) for _ in range(len(time_groups.keys()))]\n",
    "    present = [defaultdict(lambda: False) for _ in range(91)]\n",
    "    #initial_values = [[] for _ in range(len(time_groups.keys()))]\n",
    "    initial_values = [[] for _ in range(91)]\n",
    "    agent_seen = set()\n",
    "    agent_offsets = {}\n",
    "    offset = 0\n",
    "\n",
    "    #print(time_groups.keys())\n",
    "    for time in sorted(time_groups.keys()):\n",
    "        time_indicies = time_groups[time]\n",
    "        agent_ids_at_time = agent_ids[time_indicies]\n",
    "        time_index = int(time * 10)\n",
    "\n",
    "        agent_groups = defaultdict(list)\n",
    "        #[agent_groups[int(val.item())].append(idx) for idx, val in enumerate(agent_ids_at_time)]\n",
    "        for local_idx, val in enumerate(agent_ids_at_time):\n",
    "            global_idx = time_indicies[local_idx]\n",
    "            agent_groups[int(val.item())].append(global_idx)\n",
    "\n",
    "        for id, agent_indicies in agent_groups.items():\n",
    "            agent_positions_at_time = positions[agent_indicies]\n",
    "\n",
    "            #print(f'{time_index}-{len(trajectories)}')\n",
    "            if time_index < len(trajectories): # only train up to the forecast horizon\n",
    "                trajectories[time_index][id] = agent_positions_at_time\n",
    "                present[time_index][id] = True\n",
    "            \n",
    "                if id not in agent_seen:\n",
    "                    agent_seen.add(id)\n",
    "                    initial_values[time_index].append(agent_positions_at_time)\n",
    "                    num_agent_positions = agent_positions_at_time.shape[0]\n",
    "                    start = offset\n",
    "                    end = offset + num_agent_positions\n",
    "                    agent_offsets[id] = (start, end)\n",
    "                    offset = end\n",
    "\n",
    "    return trajectories, present, initial_values, agent_offsets, integration_times, list(agent_seen)\n",
    "\n",
    "def construct_agent_trajectories2(agent_ids, positions, times):\n",
    "    rounded_times = torch.round(times * 10) / 10.0\n",
    "    unique_times = torch.unique(rounded_times)\n",
    "    integration_times = unique_times[unique_times <= 9.0]\n",
    "\n",
    "    num_bins = 91\n",
    "    trajectories = [{} for _ in range(num_bins)]\n",
    "    present = [defaultdict(lambda: False) for _ in range(num_bins)]\n",
    "    initial_values = [[] for _ in range(num_bins)]\n",
    "\n",
    "    agent_seen = set()\n",
    "    agent_offsets = {}\n",
    "    offset = 0\n",
    "\n",
    "    for time_val in integration_times:\n",
    "        mask = rounded_times == time_val\n",
    "        time_indices = torch.nonzero(mask.flatten(), as_tuple=False).squeeze(1)\n",
    "        agent_ids_at_time = agent_ids[time_indices]\n",
    "        time_index = int((time_val * 10).item())\n",
    "\n",
    "        unique_ids = torch.unique(agent_ids_at_time)\n",
    "        for agent_id in unique_ids.tolist():\n",
    "            agent_mask = agent_ids_at_time == agent_id\n",
    "            agent_indices = torch.nonzero(agent_mask.flatten(), as_tuple=False).squeeze(1)\n",
    "            global_indices = time_indices[agent_indices]\n",
    "            agent_positions = positions[global_indices]\n",
    "\n",
    "            trajectories[time_index][agent_id] = agent_positions\n",
    "            present[time_index][agent_id] = True\n",
    "\n",
    "            if agent_id not in agent_seen:\n",
    "                agent_seen.add(agent_id)\n",
    "                initial_values[time_index].append(agent_positions)\n",
    "                num_agent_positions = agent_positions.shape[0]\n",
    "                start = offset\n",
    "                end = offset + num_agent_positions\n",
    "                agent_offsets[agent_id] = (start, end)\n",
    "                offset = end\n",
    "\n",
    "    return trajectories, present, initial_values, agent_offsets, integration_times, list(agent_seen)\n",
    "\n",
    "def reconstruct_trajectories(estimated_occupancy, present, agent_offsets, integration_times, agent_ids):\n",
    "    #reconstructed_trajectories = [{} for _ in range(integration_times.shape[0])]\n",
    "    reconstructed_trajectories = [{} for _ in range(91)]\n",
    "    for time_index, _ in enumerate(integration_times):\n",
    "        for id in agent_ids:\n",
    "            if present[time_index][id]:\n",
    "                start, end = agent_offsets[id]\n",
    "                estimated_occupancy_at_time = estimated_occupancy[time_index][0]\n",
    "                reconstructed_trajectories[time_index][id] = estimated_occupancy_at_time[start:end]\n",
    "    return reconstructed_trajectories\n",
    "\n",
    "def occupancy_alignment2(flow_field, scene_context,\n",
    "                         #agent_ids, positions, times):\n",
    "                         trajectories, present, initial_values, agent_offsets, integration_times, ids):\n",
    "    #trajectories, present, initial_values, agent_offsets, integration_times, ids = construct_agent_trajectories(agent_ids, positions, times)\n",
    "    estimated_occupancy = flow_field.warp_occupancy2(initial_values, integration_times, scene_context)\n",
    "    estimated_trajectories = reconstruct_trajectories(estimated_occupancy, present, agent_offsets, integration_times, ids)\n",
    "    \n",
    "    occupancy_loss = 0\n",
    "    count = 0\n",
    "\n",
    "    for time_index, _ in enumerate(integration_times):\n",
    "        for id in ids:\n",
    "            if present[time_index][id]:\n",
    "                ground_truth_positions = trajectories[time_index][id]\n",
    "                estimated_positions = estimated_trajectories[time_index][id]\n",
    "                occupancy_loss += torch.mean(torch.abs(ground_truth_positions - estimated_positions))\n",
    "                count += 1\n",
    "                \n",
    "    return occupancy_loss / count\n",
    "\n",
    "def occupancy_alignment3(flow_field, scene_context,\n",
    "                         agent_ids, positions, times):\n",
    "    trajectories, present, initial_values, agent_offsets, integration_times, ids = construct_agent_trajectories2(agent_ids, positions, times)\n",
    "    estimated_occupancy = flow_field.warp_occupancy2(initial_values, integration_times, scene_context)\n",
    "    estimated_trajectories = reconstruct_trajectories(estimated_occupancy, present, agent_offsets, integration_times, ids)\n",
    "    \n",
    "    occupancy_loss = 0\n",
    "    count = 0\n",
    "\n",
    "    for time_index, _ in enumerate(integration_times):\n",
    "        for id in ids:\n",
    "            if present[time_index][id]:\n",
    "                ground_truth_positions = trajectories[time_index][id]\n",
    "                estimated_positions = estimated_trajectories[time_index][id]\n",
    "                occupancy_loss += torch.mean(torch.abs(ground_truth_positions - estimated_positions))\n",
    "                count += 1\n",
    "                \n",
    "    return occupancy_loss / count\n",
    "\n",
    "for param in flow_field.scene_encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "optim = torch.optim.Adam(flow_field.flow_field.parameters(), lr=1e-5, weight_decay=0)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.999)\n",
    "\n",
    "# occupancy_alignment_tensors = []\n",
    "# with torch.no_grad():\n",
    "#     for road_map, agent_trajectories, flow_field_agent_ids, flow_field_positions, flow_field_times, flow_field_velocities, _, _ in scenes:\n",
    "#         road_map = road_map.to(device)\n",
    "#         agent_trajectories = agent_trajectories.to(device)\n",
    "#         p = flow_field_positions.to(device)\n",
    "#         t = flow_field_times.to(device)\n",
    "#         v = flow_field_velocities.to(device)\n",
    "    \n",
    "#         scene_context = flow_field.scene_encoder(road_map, agent_trajectories)\n",
    "#         trajectories, present, initial_values, agent_offsets, integration_times, ids = construct_agent_trajectories(flow_field_agent_ids[0], p[0], t[0])\n",
    "#         occupancy_alignment_tensors.append((trajectories, present, initial_values, agent_offsets, integration_times, ids, scene_context))\n",
    "\n",
    "EPOCHS = 1000\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_flow_loss = 0\n",
    "    epoch_occupancy_loss = 0\n",
    "    epoch_loss = 0\n",
    "    #for road_map, agent_trajectories, flow_field_agent_ids, flow_field_positions, flow_field_times, flow_field_velocities, _, _ in scenes:\n",
    "    for i in range(len(scenes)):\n",
    "        road_map, agent_trajectories, flow_field_agent_ids, flow_field_positions, flow_field_times, flow_field_velocities, _, _ = scenes[i]\n",
    "        #trajectories, present, initial_values, agent_offsets, integration_times, ids, scene_context = occupancy_alignment_tensors[i]\n",
    "\n",
    "        road_map = road_map.to(device)\n",
    "        agent_trajectories = agent_trajectories.to(device)\n",
    "        flow_field_agent_ids = flow_field_agent_ids.to(device)\n",
    "        p = flow_field_positions.to(device)\n",
    "        t = flow_field_times.to(device)\n",
    "        v = flow_field_velocities.to(device)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            scene_context = flow_field.scene_encoder(road_map, agent_trajectories)\n",
    "        flow = flow_field.flow_field(t, p, scene_context)\n",
    "\n",
    "        flow_loss = F.mse_loss(flow, v)\n",
    "\n",
    "        #occupancy_loss = occupancy_alignment2(flow_field, scene_context,\n",
    "        #                                      trajectories, present, initial_values, agent_offsets, integration_times, ids)\n",
    "        occupancy_loss = occupancy_alignment3(flow_field, scene_context, flow_field_agent_ids[0], p[0], t[0])\n",
    "\n",
    "        #print(f'ol2: {occupancy_loss}')\n",
    "        #print(f'ol3: {ol3}')\n",
    "\n",
    "        loss = flow_loss + occupancy_loss\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(flow_field.parameters(), max_norm=1.0)\n",
    "        optim.step()\n",
    "\n",
    "        epoch_flow_loss += flow_loss\n",
    "        epoch_occupancy_loss += occupancy_loss\n",
    "        epoch_loss += loss\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    epoch_flow_loss /= NUM_SCENES\n",
    "    epoch_occupancy_loss /= NUM_SCENES\n",
    "    epoch_loss /= NUM_SCENES\n",
    "    \n",
    "    #if epoch == 0 or (epoch + 1) % 100 == 0:\n",
    "    print(f'epoch {epoch+1} flow_loss: {epoch_flow_loss.item()}')\n",
    "    print(f'epoch {epoch+1} occupancy_loss: {epoch_occupancy_loss.item()}')\n",
    "    print(f'epoch {epoch+1} loss: {epoch_loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbeecf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for road_map, agent_trajectories, _, flow_field_positions, flow_field_times, _, _, _ in scenes:\n",
    "    count += 1\n",
    "    if count > MAX_SCENES_TO_RENDER:\n",
    "        break\n",
    "\n",
    "    road_map = road_map.to(device)\n",
    "    agent_trajectories = agent_trajectories.to(device)\n",
    "    p = flow_field_positions.to(device)\n",
    "    t = flow_field_times.to(device)\n",
    "    flow = flow_field(t, p, road_map, agent_trajectories)\n",
    "\n",
    "    anim = render_flow_at_spacetime(road_map[0].cpu(), flow_field_times[0].cpu(), flow_field_positions[0].cpu(), flow[0].detach().cpu())\n",
    "    display(HTML(anim.to_jshtml()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff79e7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for road_map, agent_trajectories, _, _, _, _, _, _ in scenes:\n",
    "    count += 1\n",
    "    if count > MAX_SCENES_TO_RENDER:\n",
    "        break\n",
    "\n",
    "    road_map = road_map.to(device)\n",
    "    agent_trajectories = agent_trajectories.to(device)\n",
    "    \n",
    "    scene_context = flow_field.scene_encoder(road_map, agent_trajectories)\n",
    "    anim = render_flow_field(flow_field, road_map, road_map[0].shape[0], 10, 91, 10, scene_context)\n",
    "    display(HTML(anim.to_jshtml()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76b6558",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for road_map, agent_trajectories, _, positions, times, _, _, _ in scenes:\n",
    "    count += 1\n",
    "    if count > MAX_SCENES_TO_RENDER:\n",
    "        break\n",
    "\n",
    "    road_map = road_map.to(device)\n",
    "    agent_trajectories = agent_trajectories.to(device)\n",
    "    positions = positions.to(device)\n",
    "    times = times.to(device)\n",
    "    \n",
    "    scene_context = flow_field.scene_encoder(road_map, agent_trajectories)\n",
    "    anim = render_occupancy_and_flow_unoccluded(flow_field, road_map, times, positions, 11, scene_context, f'examples/aligned_occupancy_estimate/sample{count}.gif')\n",
    "    #display(HTML(anim.to_jshtml()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ofenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
