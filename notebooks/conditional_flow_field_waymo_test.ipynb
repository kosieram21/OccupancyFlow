{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c35d59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import display, HTML\n",
    "from collections import defaultdict\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(\"__file__\"), '..')))\n",
    "from datasets.Waymo import WaymoDataset, waymo_collate_fn\n",
    "from model import OccupancyFlowNetwork\n",
    "from visualize import render_observed_scene_state, render_flow_at_spacetime, render_flow_field, render_occupancy_and_flow_unoccluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09d5231e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SCENES = 16#25\n",
    "MAX_SCENES_TO_RENDER = 0\n",
    "\n",
    "tfrecord_path = '../../data1/waymo_dataset/v1.1/waymo_open_dataset_motion_v_1_1_0/uncompressed/tf_example/validation'\n",
    "idx_path = '../../data1/waymo_dataset/v1.1/idx/validation'\n",
    "dataset = WaymoDataset(tfrecord_path, idx_path)\n",
    "dataloader = DataLoader(dataset, batch_size=NUM_SCENES, collate_fn=waymo_collate_fn)\n",
    "\n",
    "scenes = []\n",
    "#for _ in range(NUM_SCENES):\n",
    "scenes.append(next(iter(dataloader)))\n",
    "#print(scenes[-1][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0ec54fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for road_map, agent_trajectories, _, _, _, _, _, _ in scenes:\n",
    "    count += 1\n",
    "    if count > MAX_SCENES_TO_RENDER:\n",
    "        break\n",
    "    \n",
    "    render_observed_scene_state(road_map[0], agent_trajectories[0], f'examples/observed_scene_state/sample{count}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25582bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for road_map, _, _, flow_field_positions, flow_field_times, flow_field_velocities, _, _ in scenes:\n",
    "    count += 1\n",
    "    if count > MAX_SCENES_TO_RENDER:\n",
    "        break\n",
    "    \n",
    "    anim = render_flow_at_spacetime(road_map[0], flow_field_times[0], flow_field_positions[0], flow_field_velocities[0], f'examples/ground_truth/sample{count}.gif')\n",
    "    #display(HTML(anim.to_jshtml()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f99e7084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed1db3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def occupancy_alignment(flow_field, scene_context,\n",
    "                        agent_ids, positions, times):\n",
    "    occupancy_loss = 0\n",
    "    count = 0\n",
    "\n",
    "    agent_groups = defaultdict(list)\n",
    "    [agent_groups[round(val.item(), 1)].append(idx) for idx, val in enumerate(agent_ids)]\n",
    "\n",
    "    for id, agent_indices in agent_groups.items():#agent_groups.values():\n",
    "        agent_poistions = positions[agent_indices]\n",
    "        agent_times = times[agent_indices]\n",
    "        \n",
    "        time_groups = defaultdict(list)\n",
    "        [time_groups[round(val.item(), 1)].append(idx) for idx, val in enumerate(agent_times)]\n",
    "\n",
    "        occupancy = []\n",
    "        integration_times = []\n",
    "\n",
    "        for time, time_indices in time_groups.items():\n",
    "            integration_times.append(time)\n",
    "            occupancy.append(agent_poistions[time_indices])\n",
    "\n",
    "        initial_value = occupancy[0].unsqueeze(0) # TODO: unsqueeze is weird here we only do it because of ode expected shape...\n",
    "        integration_times = torch.FloatTensor(integration_times).to(times.device)\n",
    "        estimated_occupancy = flow_field.warp_occupancy(initial_value, integration_times, scene_context, use_custom=True)\n",
    "\n",
    "        for i in range(len(occupancy)):\n",
    "            occupancy_loss += torch.mean(torch.abs(estimated_occupancy[i].squeeze(0) - occupancy[i]))\n",
    "            count += 1\n",
    "            \n",
    "    return occupancy_loss / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fb227c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 loss: 0.8511417508125305\n",
      "epoch 100 loss: 0.5243859887123108\n",
      "epoch 200 loss: 0.41460734605789185\n",
      "epoch 300 loss: 0.3635062277317047\n",
      "epoch 400 loss: 0.31568121910095215\n",
      "epoch 500 loss: 0.26742982864379883\n",
      "epoch 600 loss: 0.2034892439842224\n",
      "epoch 700 loss: 0.18550574779510498\n",
      "epoch 800 loss: 0.1654248684644699\n",
      "epoch 900 loss: 0.14943347871303558\n",
      "epoch 1000 loss: 0.14423057436943054\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m loss = flow_loss\n\u001b[32m     27\u001b[39m optim.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m torch.nn.utils.clip_grad_norm_(flow_field.parameters(), max_norm=\u001b[32m1.0\u001b[39m)\n\u001b[32m     30\u001b[39m optim.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ofenv/lib/python3.11/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ofenv/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ofenv/lib/python3.11/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "flow_field = OccupancyFlowNetwork(road_map_image_size=256, road_map_window_size=8, \n",
    "                                  trajectory_feature_dim=10, \n",
    "                                  embedding_dim=256, \n",
    "                                  flow_field_hidden_dim=256, flow_field_fourier_features=0).to(device)\n",
    "flow_field.train()\n",
    "\n",
    "\n",
    "optim = torch.optim.Adam(flow_field.parameters(), lr=1e-4, weight_decay=0)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.999)\n",
    "\n",
    "EPOCHS = 1000\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    for road_map, agent_trajectories, flow_field_agent_ids, flow_field_positions, flow_field_times, flow_field_velocities, _, _ in scenes:\n",
    "        road_map = road_map.to(device)\n",
    "        agent_trajectories = agent_trajectories.to(device)\n",
    "        p = flow_field_positions.to(device)\n",
    "        t = flow_field_times.to(device)\n",
    "        v = flow_field_velocities.to(device)\n",
    "    \n",
    "        scene_context = flow_field.scene_encoder(road_map, agent_trajectories)\n",
    "        flow = flow_field.flow_field(t, p, scene_context)\n",
    "\n",
    "        flow_loss = F.mse_loss(flow, v)\n",
    "        loss = flow_loss\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(flow_field.parameters(), max_norm=1.0)\n",
    "        optim.step()\n",
    "\n",
    "        epoch_loss += loss\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    epoch_loss /= NUM_SCENES\n",
    "    \n",
    "    if epoch == 0 or (epoch + 1) % 100 == 0:\n",
    "        print(f'epoch {epoch+1} loss: {epoch_loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af5f2e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for road_map, agent_trajectories, _, positions, times, _, _, _ in scenes:\n",
    "    count += 1\n",
    "    if count > MAX_SCENES_TO_RENDER:\n",
    "        break\n",
    "\n",
    "    road_map = road_map.to(device)\n",
    "    agent_trajectories = agent_trajectories.to(device)\n",
    "    positions = positions.to(device)\n",
    "    times = times.to(device)\n",
    "    \n",
    "    scene_context = flow_field.scene_encoder(road_map, agent_trajectories)\n",
    "    anim = render_occupancy_and_flow_unoccluded(flow_field, road_map, times, positions, 11, scene_context, f'examples/unaligned_occupancy_estimate/sample{count}.gif')\n",
    "    #display(HTML(anim.to_jshtml()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af75653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: I wonder if this should be part of the waymo collate function\n",
    "def construct_agent_trajectories(agent_ids, positions, times):\n",
    "    #print(agent_ids.shape)\n",
    "    #print(positions.shape)\n",
    "    #print(times.shape)\n",
    "    time_groups = defaultdict(list)\n",
    "    [time_groups[round(val.item(), 1)].append(idx) for idx, val in enumerate(times)]\n",
    "    integration_times = torch.FloatTensor(sorted([t for t in time_groups.keys() if t <= 9.0])).to(times.device)\n",
    "\n",
    "    #trajectories = [{} for _ in range(len(time_groups.keys()))]\n",
    "    trajectories = [{} for _ in range(91)]\n",
    "    #present = [defaultdict(lambda: False) for _ in range(len(time_groups.keys()))]\n",
    "    present = [defaultdict(lambda: False) for _ in range(91)]\n",
    "    #initial_values = [[] for _ in range(len(time_groups.keys()))]\n",
    "    initial_values = [[] for _ in range(91)]\n",
    "    agent_seen = set()\n",
    "    agent_offsets = {}\n",
    "    offset = 0\n",
    "\n",
    "    #print(time_groups.keys())\n",
    "    for time in sorted(time_groups.keys()):\n",
    "        time_indicies = time_groups[time]\n",
    "        agent_ids_at_time = agent_ids[time_indicies]\n",
    "        time_index = int(time * 10)\n",
    "\n",
    "        agent_groups = defaultdict(list)\n",
    "        #[agent_groups[int(val.item())].append(idx) for idx, val in enumerate(agent_ids_at_time)]\n",
    "        for local_idx, val in enumerate(agent_ids_at_time):\n",
    "            global_idx = time_indicies[local_idx]\n",
    "            agent_groups[int(val.item())].append(global_idx)\n",
    "\n",
    "        for id, agent_indicies in agent_groups.items():\n",
    "            agent_positions_at_time = positions[agent_indicies]\n",
    "\n",
    "            #print(f'{time_index}-{len(trajectories)}')\n",
    "            if time_index < len(trajectories): # only train up to the forecast horizon\n",
    "                trajectories[time_index][id] = agent_positions_at_time\n",
    "                present[time_index][id] = True\n",
    "            \n",
    "                if id not in agent_seen:\n",
    "                    agent_seen.add(id)\n",
    "                    initial_values[time_index].append(agent_positions_at_time)\n",
    "                    num_agent_positions = agent_positions_at_time.shape[0]\n",
    "                    start = offset\n",
    "                    end = offset + num_agent_positions\n",
    "                    agent_offsets[id] = (start, end)\n",
    "                    offset = end\n",
    "\n",
    "    return trajectories, present, initial_values, agent_offsets, integration_times, list(agent_seen)\n",
    "\n",
    "def reconstruct_trajectories(estimated_occupancy, present, agent_offsets, integration_times, agent_ids):\n",
    "    #print(integration_times.shape)\n",
    "    #print(agent_ids)\n",
    "    #reconstructed_trajectories = [{} for _ in range(integration_times.shape[0])]\n",
    "    reconstructed_trajectories = [{} for _ in range(91)]\n",
    "    for time_index, _ in enumerate(integration_times):\n",
    "        for id in agent_ids:\n",
    "            if present[time_index][id]:\n",
    "                start, end = agent_offsets[id]\n",
    "                estimated_occupancy_at_time = estimated_occupancy[time_index][0] # remove batch dim\n",
    "                reconstructed_trajectories[time_index][id] = estimated_occupancy_at_time[start:end]\n",
    "    return reconstructed_trajectories\n",
    "\n",
    "def occupancy_alignment2(flow_field, scene_context,\n",
    "                         #agent_ids, positions, times):\n",
    "                         trajectories, present, initial_values, agent_offsets, integration_times, ids):\n",
    "    #trajectories, present, initial_values, agent_offsets, integration_times, ids = construct_agent_trajectories(agent_ids, positions, times)\n",
    "    estimated_occupancy = flow_field.warp_occupancy2(initial_values, integration_times, scene_context)\n",
    "    estimated_trajectories = reconstruct_trajectories(estimated_occupancy, present, agent_offsets, integration_times, ids)\n",
    "    \n",
    "    occupancy_loss = 0\n",
    "    count = 0\n",
    "\n",
    "    for time_index, _ in enumerate(integration_times):\n",
    "        for id in ids:\n",
    "            if present[time_index][id]:\n",
    "                ground_truth_positions = trajectories[time_index][id]\n",
    "                estimated_positions = estimated_trajectories[time_index][id]\n",
    "                occupancy_loss += torch.mean(torch.abs(ground_truth_positions - estimated_positions))\n",
    "                count += 1\n",
    "                \n",
    "    return occupancy_loss / count\n",
    "\n",
    "for param in flow_field.scene_encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "optim = torch.optim.Adam(flow_field.flow_field.parameters(), lr=1e-5, weight_decay=0)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.999)\n",
    "\n",
    "occupancy_alignment_tensors = []\n",
    "for road_map, agent_trajectories, flow_field_agent_ids, flow_field_positions, flow_field_times, flow_field_velocities, _, _ in scenes:\n",
    "    road_map = road_map.to(device)\n",
    "    agent_trajectories = agent_trajectories.to(device)\n",
    "    p = flow_field_positions.to(device)\n",
    "    t = flow_field_times.to(device)\n",
    "    v = flow_field_velocities.to(device)\n",
    "    \n",
    "    scene_context = flow_field.scene_encoder(road_map, agent_trajectories)\n",
    "    trajectories, present, initial_values, agent_offsets, integration_times, ids = construct_agent_trajectories(flow_field_agent_ids[0], p[0], t[0])\n",
    "    occupancy_alignment_tensors.append((trajectories, present, initial_values, agent_offsets, integration_times, ids, scene_context))\n",
    "\n",
    "EPOCHS = 1000\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_flow_loss = 0\n",
    "    epoch_occupancy_loss = 0\n",
    "    epoch_loss = 0\n",
    "    #for road_map, agent_trajectories, flow_field_agent_ids, flow_field_positions, flow_field_times, flow_field_velocities, _, _ in scenes:\n",
    "    for i in range(len(scenes)):\n",
    "        road_map, agent_trajectories, flow_field_agent_ids, flow_field_positions, flow_field_times, flow_field_velocities, _, _ = scenes[i]\n",
    "        trajectories, present, initial_values, agent_offsets, integration_times, ids, scene_context = occupancy_alignment_tensors[i]\n",
    "\n",
    "        road_map = road_map.to(device)\n",
    "        agent_trajectories = agent_trajectories.to(device)\n",
    "        p = flow_field_positions.to(device)\n",
    "        t = flow_field_times.to(device)\n",
    "        v = flow_field_velocities.to(device)\n",
    "    \n",
    "        #scene_context = flow_field.scene_encoder(road_map, agent_trajectories)\n",
    "        flow = flow_field.flow_field(t, p, scene_context)\n",
    "\n",
    "        flow_loss = F.mse_loss(flow, v)\n",
    "\n",
    "        occupancy_loss = occupancy_alignment2(flow_field, scene_context,\n",
    "                                              trajectories, present, initial_values, agent_offsets, integration_times, ids)\n",
    "\n",
    "        loss = flow_loss + occupancy_loss\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(flow_field.parameters(), max_norm=1.0)\n",
    "        optim.step()\n",
    "\n",
    "        epoch_flow_loss += flow_loss\n",
    "        epoch_occupancy_loss += occupancy_loss\n",
    "        epoch_loss += loss\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    epoch_flow_loss /= NUM_SCENES\n",
    "    epoch_occupancy_loss /= NUM_SCENES\n",
    "    epoch_loss /= NUM_SCENES\n",
    "    \n",
    "    #if epoch == 0 or (epoch + 1) % 100 == 0:\n",
    "    print(f'epoch {epoch+1} flow_loss: {epoch_flow_loss.item()}')\n",
    "    print(f'epoch {epoch+1} occupancy_loss: {epoch_occupancy_loss.item()}')\n",
    "    print(f'epoch {epoch+1} loss: {epoch_loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbeecf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for road_map, agent_trajectories, _, flow_field_positions, flow_field_times, _, _, _ in scenes:\n",
    "    count += 1\n",
    "    if count > MAX_SCENES_TO_RENDER:\n",
    "        break\n",
    "\n",
    "    road_map = road_map.to(device)\n",
    "    agent_trajectories = agent_trajectories.to(device)\n",
    "    p = flow_field_positions.to(device)\n",
    "    t = flow_field_times.to(device)\n",
    "    flow = flow_field(t, p, road_map, agent_trajectories)\n",
    "\n",
    "    anim = render_flow_at_spacetime(road_map[0].cpu(), flow_field_times[0].cpu(), flow_field_positions[0].cpu(), flow[0].detach().cpu())\n",
    "    display(HTML(anim.to_jshtml()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff79e7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for road_map, agent_trajectories, _, _, _, _, _, _ in scenes:\n",
    "    count += 1\n",
    "    if count > MAX_SCENES_TO_RENDER:\n",
    "        break\n",
    "\n",
    "    road_map = road_map.to(device)\n",
    "    agent_trajectories = agent_trajectories.to(device)\n",
    "    \n",
    "    scene_context = flow_field.scene_encoder(road_map, agent_trajectories)\n",
    "    anim = render_flow_field(flow_field, road_map, road_map[0].shape[0], 10, 91, 10, scene_context)\n",
    "    display(HTML(anim.to_jshtml()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76b6558",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for road_map, agent_trajectories, _, positions, times, _, _, _ in scenes:\n",
    "    count += 1\n",
    "    if count > MAX_SCENES_TO_RENDER:\n",
    "        break\n",
    "\n",
    "    road_map = road_map.to(device)\n",
    "    agent_trajectories = agent_trajectories.to(device)\n",
    "    positions = positions.to(device)\n",
    "    times = times.to(device)\n",
    "    \n",
    "    scene_context = flow_field.scene_encoder(road_map, agent_trajectories)\n",
    "    anim = render_occupancy_and_flow_unoccluded(flow_field, road_map, times, positions, 11, scene_context, f'examples/aligned_occupancy_estimate/sample{count}.gif')\n",
    "    #display(HTML(anim.to_jshtml()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ofenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
