{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c35d59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import display, HTML\n",
    "from collections import defaultdict\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(\"__file__\"), '..')))\n",
    "from datasets.Waymo import WaymoDataset, waymo_collate_fn\n",
    "from model import OccupancyFlowNetwork\n",
    "from visualize import render_observed_scene_state, render_flow_at_spacetime, render_flow_field, render_occupancy_and_flow_unoccluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d5231e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SCENES = 1\n",
    "MAX_SCENES_TO_RENDER = 1\n",
    "\n",
    "tfrecord_path = '../../data1/waymo_dataset/v1.1/waymo_open_dataset_motion_v_1_1_0/uncompressed/tf_example/validation'\n",
    "idx_path = '../../data1/waymo_dataset/v1.1/idx/validation'\n",
    "dataset = WaymoDataset(tfrecord_path, idx_path)\n",
    "dataloader = DataLoader(dataset, batch_size=1, collate_fn=waymo_collate_fn)\n",
    "\n",
    "scenes = []\n",
    "for _ in range(NUM_SCENES):\n",
    "    scenes.append(next(iter(dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ec54fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for road_map, agent_trajectories, _, _, _, _, _, _ in scenes:\n",
    "    count += 1\n",
    "    if count > MAX_SCENES_TO_RENDER:\n",
    "        break\n",
    "    \n",
    "    render_observed_scene_state(road_map[0], agent_trajectories[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25582bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for road_map, _, _, flow_field_positions, flow_field_times, flow_field_velocities, _, _ in scenes:\n",
    "    count += 1\n",
    "    if count > MAX_SCENES_TO_RENDER:\n",
    "        break\n",
    "    \n",
    "    anim = render_flow_at_spacetime(road_map[0], flow_field_times[0], flow_field_positions[0], flow_field_velocities[0])\n",
    "    display(HTML(anim.to_jshtml()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99e7084",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1db3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def occupancy_alignment(flow_field, scene_context,\n",
    "                        agent_ids, positions, times):\n",
    "    occupancy_loss = 0\n",
    "    count = 0\n",
    "\n",
    "    gt_traj = defaultdict(list)\n",
    "    agent_groups = defaultdict(list)\n",
    "    [agent_groups[round(val.item(), 1)].append(idx) for idx, val in enumerate(agent_ids)]\n",
    "\n",
    "    for id, agent_indices in agent_groups.items():#agent_groups.values():\n",
    "        agent_poistions = positions[agent_indices]\n",
    "        agent_times = times[agent_indices]\n",
    "        \n",
    "        time_groups = defaultdict(list)\n",
    "        [time_groups[round(val.item(), 1)].append(idx) for idx, val in enumerate(agent_times)]\n",
    "\n",
    "        occupancy = []\n",
    "        integration_times = []\n",
    "\n",
    "        for time, time_indices in time_groups.items():\n",
    "            gt_traj[id].append((int(time * 10), torch.mean(agent_poistions[time_indices])))\n",
    "            integration_times.append(time)\n",
    "            occupancy.append(agent_poistions[time_indices])\n",
    "\n",
    "        initial_value = occupancy[0].unsqueeze(0) # TODO: unsqueeze is weird here we only do it because of ode expected shape...\n",
    "        integration_times = torch.FloatTensor(integration_times).to(times.device)\n",
    "        estimated_occupancy = flow_field.warp_occupancy(initial_value, integration_times, scene_context, use_custom=True)\n",
    "\n",
    "        for i in range(len(occupancy)):\n",
    "            occupancy_loss += torch.mean(torch.abs(estimated_occupancy[i].squeeze(0) - occupancy[i]))\n",
    "            count += 1\n",
    "        \n",
    "    for id in agent_groups.keys():\n",
    "        print(f'>>>>>>>>>{id}<<<<<<<<<')\n",
    "        for time_index, mean_pos in gt_traj[id]:\n",
    "            print(f'{time_index}, {mean_pos}')\n",
    "        print('-----------------------')\n",
    "            \n",
    "    return occupancy_loss / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fb227c",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_field = OccupancyFlowNetwork(road_map_image_size=256, road_map_window_size=8, \n",
    "                                  trajectory_feature_dim=10, \n",
    "                                  embedding_dim=256, \n",
    "                                  flow_field_hidden_dim=256, flow_field_fourier_features=0).to(device)\n",
    "flow_field.train()\n",
    "\n",
    "\n",
    "optim = torch.optim.Adam(flow_field.parameters(), lr=1e-4, weight_decay=0)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.999)\n",
    "\n",
    "EPOCHS = 1000\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    for road_map, agent_trajectories, flow_field_agent_ids, flow_field_positions, flow_field_times, flow_field_velocities, _, _ in scenes:\n",
    "        road_map = road_map.to(device)\n",
    "        agent_trajectories = agent_trajectories.to(device)\n",
    "        p = flow_field_positions.to(device)\n",
    "        t = flow_field_times.to(device)\n",
    "        v = flow_field_velocities.to(device)\n",
    "    \n",
    "        scene_context = flow_field.scene_encoder(road_map, agent_trajectories)\n",
    "        flow = flow_field.flow_field(t, p, scene_context)\n",
    "\n",
    "        flow_loss = F.mse_loss(flow, v)\n",
    "        loss = flow_loss\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(flow_field.parameters(), max_norm=1.0)\n",
    "        optim.step()\n",
    "\n",
    "        epoch_loss += loss\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    epoch_loss /= NUM_SCENES\n",
    "    \n",
    "    if epoch == 0 or (epoch + 1) % 100 == 0:\n",
    "        print(f'epoch {epoch+1} loss: {epoch_loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af75653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: I wonder if this should be part of the waymo collate function\n",
    "def construct_agent_trajectories(agent_ids, positions, times):\n",
    "    #print(agent_ids.shape)\n",
    "    #print(positions.shape)\n",
    "    #print(times.shape)\n",
    "    time_groups = defaultdict(list)\n",
    "    [time_groups[round(val.item(), 1)].append(idx) for idx, val in enumerate(times)]\n",
    "    integration_times = torch.FloatTensor(sorted([t for t in time_groups.keys()])).to(times.device)\n",
    "\n",
    "    #trajectories = [{} for _ in range(len(time_groups.keys()))]\n",
    "    trajectories = [{} for _ in range(91)]\n",
    "    #present = [defaultdict(lambda: False) for _ in range(len(time_groups.keys()))]\n",
    "    present = [defaultdict(lambda: False) for _ in range(91)]\n",
    "    #initial_values = [[] for _ in range(len(time_groups.keys()))]\n",
    "    initial_values = [[] for _ in range(91)]\n",
    "    agent_seen = set()\n",
    "    agent_offsets = {}\n",
    "    offset = 0\n",
    "\n",
    "    for time in sorted(time_groups.keys()):\n",
    "        time_indicies = time_groups[time]\n",
    "        agent_ids_at_time = agent_ids[time_indicies]\n",
    "        time_index = int(time * 10)\n",
    "\n",
    "        agent_groups = defaultdict(list)\n",
    "        # This was the problem... why????\n",
    "        #[agent_groups[int(val.item())].append(idx) for idx, val in enumerate(agent_ids_at_time)]\n",
    "        for local_idx, val in enumerate(agent_ids_at_time):\n",
    "            global_idx = time_indicies[local_idx]\n",
    "            agent_groups[int(val.item())].append(global_idx)\n",
    "\n",
    "        for id, agent_indicies in agent_groups.items():\n",
    "            agent_positions_at_time = positions[agent_indicies]\n",
    "\n",
    "            trajectories[time_index][id] = agent_positions_at_time\n",
    "            present[time_index][id] = True\n",
    "            \n",
    "            if id not in agent_seen:\n",
    "                agent_seen.add(id)\n",
    "                initial_values[time_index].append(agent_positions_at_time)\n",
    "                num_agent_positions = agent_positions_at_time.shape[0]\n",
    "                start = offset\n",
    "                end = offset + num_agent_positions\n",
    "                agent_offsets[id] = (start, end)\n",
    "                offset = end\n",
    "\n",
    "    return trajectories, present, initial_values, agent_offsets, integration_times, list(agent_seen)\n",
    "\n",
    "def reconstruct_trajectories(estimated_occupancy, present, agent_offsets, integration_times, agent_ids):\n",
    "    #print(integration_times.shape)\n",
    "    #print(agent_ids)\n",
    "    #reconstructed_trajectories = [{} for _ in range(integration_times.shape[0])]\n",
    "    reconstructed_trajectories = [{} for _ in range(91)]\n",
    "    for time_index, _ in enumerate(integration_times):\n",
    "        for id in agent_ids:\n",
    "            if present[time_index][id]:\n",
    "                start, end = agent_offsets[id]\n",
    "                estimated_occupancy_at_time = estimated_occupancy[time_index][0] # remove batch dim\n",
    "                reconstructed_trajectories[time_index][id] = estimated_occupancy_at_time[start:end]\n",
    "    return reconstructed_trajectories\n",
    "\n",
    "def occupancy_alignment2(flow_field, scene_context,\n",
    "                         agent_ids, positions, times):\n",
    "    trajectories, present, initial_values, agent_offsets, integration_times, ids = construct_agent_trajectories(agent_ids, positions, times)\n",
    "    estimated_occupancy = flow_field.warp_occupancy2(initial_values, integration_times, scene_context)\n",
    "    estimated_trajectories = reconstruct_trajectories(estimated_occupancy, present, agent_offsets, integration_times, ids)\n",
    "    \n",
    "    occupancy_loss = 0\n",
    "    count = 0\n",
    "\n",
    "    gt_traj = defaultdict(list)\n",
    "    for time_index, _ in enumerate(integration_times):\n",
    "        for id in ids:\n",
    "            if present[time_index][id]:\n",
    "                ground_truth_positions = trajectories[time_index][id]\n",
    "                estimated_positions = estimated_trajectories[time_index][id]\n",
    "                gt_traj[id].append((time_index, torch.mean(ground_truth_positions)))\n",
    "                occupancy_loss += torch.mean(torch.abs(ground_truth_positions - estimated_positions))\n",
    "                count += 1\n",
    "                \n",
    "    return occupancy_loss / count\n",
    "\n",
    "for param in flow_field.scene_encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "optim = torch.optim.Adam(flow_field.flow_field.parameters(), lr=1e-5, weight_decay=0)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.999)\n",
    "\n",
    "EPOCHS = 1#000\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_flow_loss = 0\n",
    "    epoch_occupancy_loss = 0\n",
    "    epoch_loss = 0\n",
    "    for road_map, agent_trajectories, flow_field_agent_ids, flow_field_positions, flow_field_times, flow_field_velocities, _, _ in scenes:\n",
    "        road_map = road_map.to(device)\n",
    "        agent_trajectories = agent_trajectories.to(device)\n",
    "        p = flow_field_positions.to(device)\n",
    "        t = flow_field_times.to(device)\n",
    "        v = flow_field_velocities.to(device)\n",
    "    \n",
    "        scene_context = flow_field.scene_encoder(road_map, agent_trajectories)\n",
    "        flow = flow_field.flow_field(t, p, scene_context)\n",
    "\n",
    "        flow_loss = F.mse_loss(flow, v)\n",
    "\n",
    "        occupancy_loss = occupancy_alignment2(flow_field, scene_context,\n",
    "                                              flow_field_agent_ids[0], p[0], t[0])\n",
    "\n",
    "        loss = flow_loss + occupancy_loss\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(flow_field.parameters(), max_norm=1.0)\n",
    "        optim.step()\n",
    "\n",
    "        epoch_flow_loss += flow_loss\n",
    "        epoch_occupancy_loss += occupancy_loss\n",
    "        epoch_loss += loss\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    epoch_flow_loss /= NUM_SCENES\n",
    "    epoch_occupancy_loss /= NUM_SCENES\n",
    "    epoch_loss /= NUM_SCENES\n",
    "    \n",
    "    #if epoch == 0 or (epoch + 1) % 100 == 0:\n",
    "    print(f'epoch {epoch+1} flow_loss: {epoch_flow_loss.item()}')\n",
    "    print(f'epoch {epoch+1} occupancy_loss: {epoch_occupancy_loss.item()}')\n",
    "    print(f'epoch {epoch+1} loss: {epoch_loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbeecf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for road_map, agent_trajectories, _, flow_field_positions, flow_field_times, _, _, _ in scenes:\n",
    "    count += 1\n",
    "    if count > MAX_SCENES_TO_RENDER:\n",
    "        break\n",
    "\n",
    "    road_map = road_map.to(device)\n",
    "    agent_trajectories = agent_trajectories.to(device)\n",
    "    p = flow_field_positions.to(device)\n",
    "    t = flow_field_times.to(device)\n",
    "    flow = flow_field(t, p, road_map, agent_trajectories)\n",
    "\n",
    "    anim = render_flow_at_spacetime(road_map[0].cpu(), flow_field_times[0].cpu(), flow_field_positions[0].cpu(), flow[0].detach().cpu())\n",
    "    display(HTML(anim.to_jshtml()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff79e7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for road_map, agent_trajectories, _, _, _, _, _, _ in scenes:\n",
    "    count += 1\n",
    "    if count > MAX_SCENES_TO_RENDER:\n",
    "        break\n",
    "\n",
    "    road_map = road_map.to(device)\n",
    "    agent_trajectories = agent_trajectories.to(device)\n",
    "    \n",
    "    scene_context = flow_field.scene_encoder(road_map, agent_trajectories)\n",
    "    anim = render_flow_field(flow_field, road_map, road_map[0].shape[0], 10, 91, 10, scene_context)\n",
    "    display(HTML(anim.to_jshtml()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76b6558",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for road_map, agent_trajectories, _, positions, times, _, _, _ in scenes:\n",
    "    count += 1\n",
    "    if count > MAX_SCENES_TO_RENDER:\n",
    "        break\n",
    "\n",
    "    road_map = road_map.to(device)\n",
    "    agent_trajectories = agent_trajectories.to(device)\n",
    "    positions = positions.to(device)\n",
    "    times = times.to(device)\n",
    "    \n",
    "    scene_context = flow_field.scene_encoder(road_map, agent_trajectories)\n",
    "    anim = render_occupancy_and_flow_unoccluded(flow_field, road_map, times, positions, 11, scene_context)\n",
    "    display(HTML(anim.to_jshtml()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ofenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
